from pyspark.sql import SparkSession
from pyspark.sql import functions as fun
from pyspark.sql.window import Window as win

bank_data = [
    ('1/5/2025',	26000),
('1/6/2025',	26000),
('1/7/2025',	26000),
('1/8/2025',	30000),
('1/9/2025',	30000),
('1/10/2025',	28000),
('1/11/2025',	32000),
('1/12/2025',	32000),
('1/13/2025',	32000),
('1/14/2025',	32000),
('1/15/2025',	32000),
('1/16/2025',	32000),
('1/17/2025',	26000),
('1/18/2025',	26000),
('1/19/2025',	26000),
('1/20/2025',	26000),
('1/21/2025',	33000),
('1/22/2025',	33000)
]

columns = ["Date","Balance"]

spark=SparkSession.builder.getOrCreate()

balance_df=spark.createDataFrame(data=bank_data,schema=columns)

split_date = fun.split(balance_df["Date"],"/")
balance_df=balance_df.withColumn("balance_date",fun.make_date(split_date.getItem(2),split_date.getItem(0),split_date.getItem(1)))

win_spec=win.partitionBy(fun.col("Balance")).orderBy(fun.col("balance_date"))
balance_df=balance_df.withColumn("rn",fun.row_number().over(win_spec))
balance_df=balance_df.withColumn("balance_grp",fun.col("balance_date")-fun.col("rn"))

balance_grp_df=balance_df.groupBy("Balance","balance_grp").agg(fun.min("balance_date").alias("start_date"),fun.max("balance_date").alias("end_date"))
balance_grp_df=balance_grp_df.withColumnRenamed("start_date","Start_Date").withColumnRenamed("end_date","End_Date")
balance_grp_df=balance_grp_df.select("Balance","Start_Date","End_Date")

balance_grp_df.orderBy("Start_Date").show()
